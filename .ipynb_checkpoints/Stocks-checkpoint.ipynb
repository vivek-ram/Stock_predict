{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "5X_A57FFGt-f",
    "outputId": "5106eb52-17bc-41d5-e86a-29585168517f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yahoo_fin import stock_info as si\n",
    "from collections import deque\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import os\n",
    "import time\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mmbSmqQGGyZw"
   },
   "outputs": [],
   "source": [
    "def shuf(a,b):\n",
    "    # shuffle two arrays in the same way\n",
    "    state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(state)\n",
    "    np.random.shuffle(b)\n",
    "    \n",
    "\n",
    "\n",
    "def create_data(ticker,pred,Feature_columns,split,shuffle,scale,date_split,n_steps):\n",
    "    '''\n",
    "    ticker = stock symbol\n",
    "    Feature_columns = columns we need for neural net\n",
    "    predict = number of days of prediction we need\n",
    "    split = 0.2 spliting 80% to train and 20% to test\n",
    "    \n",
    "    ----------------\n",
    "    hyper-parameter\n",
    "    ----------------\n",
    "    n_steps = step size we need to use\n",
    "    shuffle = shuffle the dataset\n",
    "    scale = making the values of feature columns in range of between 0 and 1\n",
    "    \n",
    "    '''\n",
    "    if isinstance(ticker, str):\n",
    "        df = si.get_data(ticker)\n",
    "    elif isinstance(ticker , pd.DataFrame):\n",
    "        df = ticker\n",
    "    else:\n",
    "        raise TypeError(\"ticker can be a str or a `pd.DataFrame` only\")\n",
    "        \n",
    "    '''\n",
    "    we are copying following things in dictionary so it is easy for use later\n",
    "    df = dataframe\n",
    "    scaler\n",
    "    \n",
    "    \n",
    "    '''    \n",
    "    data = {}\n",
    "    data[\"df\"] = df.copy()\n",
    "    \n",
    "    for col in Feature_columns:\n",
    "        assert col in df.columns,f\"'{col}' does not exist in the dataframe.\"\n",
    "        \n",
    "    if \"date\" not in df.columns:\n",
    "        df[\"date\"] = df.index\n",
    "        \n",
    "    '''\n",
    "    ------------\n",
    "    Scaling\n",
    "    -------------\n",
    "    '''\n",
    "    if scale:\n",
    "        col_scaler = {}\n",
    "        for col in Feature_columns:\n",
    "            scaler = preprocessing.MinMaxScaler()\n",
    "            df[col] = scaler.fit_transform(np.expand_dims(df[col].values,axis=1))\n",
    "            col_scaler[col] = scaler\n",
    "        \n",
    "        data[\"col_scaler\"] = col_scaler\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    ------\n",
    "    Future target label and droping nan values\n",
    "    ------\n",
    "    '''\n",
    "    df['future'] = df['adjclose'].shift(-pred)\n",
    "    last_sequence = np.array(df[Feature_columns].tail(pred))\n",
    "    \n",
    "    df.dropna(inplace = True)\n",
    "    sequence_data = []\n",
    "    sequences = deque(maxlen=n_steps)\n",
    "    \n",
    "    for entry,target in zip(df[Feature_columns+[\"date\"]].values,df['future'].values):\n",
    "        sequences.append(entry)\n",
    "        if len(sequences) == n_steps:\n",
    "            sequence_data.append([np.array(sequences),target])\n",
    "            \n",
    "    last_sequence = list([a[:len(Feature_columns)] for a in sequences]) + list(last_sequence)\n",
    "    last_sequence = np.array(last_sequence).astype(np.float32)\n",
    "    data['last_sequence'] = last_sequence\n",
    "    \n",
    "    '''\n",
    "    creating train and test set\n",
    "    '''\n",
    "    X , y = [], []\n",
    "    for seq , target in sequence_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "        \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    if date_split:\n",
    "        '''\n",
    "        spliting by date\n",
    "        '''\n",
    "        train_samples = int((1 - split) * len(X))\n",
    "        data[\"X_train\"] = X[:train_samples]\n",
    "        data[\"y_train\"] = y[:train_samples]\n",
    "        data[\"X_test\"]  = X[train_samples:]\n",
    "        data[\"y_test\"]  = y[train_samples:]\n",
    "        \n",
    "        if shuffle:\n",
    "            # shuffle the dataset\n",
    "            shuf(data[\"X_train\"], data[\"y_train\"])\n",
    "            shuf(data[\"X_test\"], data[\"y_test\"])\n",
    "            \n",
    "    else:\n",
    "        '''\n",
    "        spliting randomly\n",
    "        '''\n",
    "        data[\"X_train\"], data[\"X_test\"], data[\"Y_train\"], data[\"Y_test\"] = train_test_split(X, y,test_size=0.2, shuffle=True)\n",
    "        \n",
    "        \n",
    "    dates = data[\"X_test\"][:,-1,-1]\n",
    "    data[\"test_df\"] = data[\"df\"].loc[dates]\n",
    "    # remove duplicated dates in the testing dataframe\n",
    "    data[\"test_df\"] = data[\"test_df\"][~data[\"test_df\"].index.duplicated(keep='first')]\n",
    "    data[\"X_train\"] = data[\"X_train\"][:, :, :len(Feature_columns)].astype(np.float32)\n",
    "    data[\"X_test\"] = data[\"X_test\"][:, :, :len(Feature_columns)].astype(np.float32)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "HXYXWItsG-iV"
   },
   "outputs": [],
   "source": [
    "def create_model(sequence_length, n_features, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
    "                loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=False):\n",
    "    model = Sequential()\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            # first layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True), batch_input_shape=(None, sequence_length, n_features)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True, batch_input_shape=(None, sequence_length, n_features)))\n",
    "        elif i == n_layers - 1:\n",
    "            # last layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=False)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=False))\n",
    "        else:\n",
    "            # hidden layers\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True))\n",
    "        # add dropout after each layer\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8WzRXzb1HCVd"
   },
   "outputs": [],
   "source": [
    "ticker = 'TSLA'\n",
    "pred = 3\n",
    "EPOCHS = 2\n",
    "# ----------------------------------------------\n",
    "split = 0.2\n",
    "shuffle=True\n",
    "feature_columns = ['adjclose', 'volume', 'open', 'high', 'low']\n",
    "\n",
    "\n",
    "scale=True\n",
    "date_split = True\n",
    "\n",
    "\n",
    "\n",
    "n_steps=30\n",
    "N_LAYERS = 2\n",
    "# LSTM cell\n",
    "CELL = LSTM\n",
    "# 256 LSTM neurons\n",
    "UNITS = 256\n",
    "# 40% dropout\n",
    "DROPOUT = 0.4\n",
    "# whether to use bidirectional RNNs\n",
    "BIDIRECTIONAL = True\n",
    "### training parameters\n",
    "\n",
    "LOSS = \"huber_loss\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 128\n",
    "model_name='stock'\n",
    "\n",
    "if BIDIRECTIONAL:\n",
    "    model_name += \"-b\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sPsclQpxHF5P",
    "outputId": "376a1330-dd02-42df-e960-f32e21e3b301",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "17/17 - 12s - loss: 3.1281e-04 - mean_absolute_error: 0.0183 - val_loss: 0.0052 - val_mean_absolute_error: 0.0593\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00523, saving model to results/stock-b.h5\n",
      "Epoch 2/2\n",
      "17/17 - 10s - loss: 4.2163e-05 - mean_absolute_error: 0.0069 - val_loss: 0.0045 - val_mean_absolute_error: 0.0531\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00523 to 0.00445, saving model to results/stock-b.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x169dea370>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "data = create_data(ticker,pred,feature_columns,split,date_split,scale,shuffle,n_steps)\n",
    "\n",
    "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
    "\n",
    "model = create_model(n_steps, len(feature_columns), loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)\n",
    "\n",
    "model.fit(data[\"X_train\"], data[\"y_train\"],batch_size=BATCH_SIZE,epochs=EPOCHS,validation_data=(data[\"X_test\"], data[\"y_test\"]),verbose=2,callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "iwNR9AV2IBu-"
   },
   "outputs": [],
   "source": [
    "def predict(model, data):\n",
    "    # retrieve the last sequence from data\n",
    "    last_sequence = data[\"last_sequence\"][-n_steps:]\n",
    "    # expand dimension\n",
    "    last_sequence = np.expand_dims(last_sequence, axis=0)\n",
    "    # get the prediction (scaled from 0 to 1)\n",
    "    prediction = model.predict(last_sequence)\n",
    "    # get the price (by inverting the scaling)\n",
    "    if scale:\n",
    "        predicted_price = data[\"col_scaler\"][\"adjclose\"].inverse_transform(prediction)[0][0]\n",
    "    else:\n",
    "        predicted_price = prediction[0][0]\n",
    "    return predicted_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_Hlipmr_IO71"
   },
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, mae = model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0)\n",
    "# calculate the mean absolute error (inverse scaling)\n",
    "if scale:\n",
    "    mean_absolute_error = data[\"col_scaler\"][\"adjclose\"].inverse_transform([[mae]])[0][0]\n",
    "else:\n",
    "    mean_absolute_error = mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "hjROldFaIeJl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "472.01608"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict the future price\n",
    "future_price = predict(model, data)\n",
    "future_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Utz-RX1JSKe",
    "outputId": "15592a3e-6f4f-4e26-edc3-c901633c7506"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future price after 3 days is 472.02$\n",
      "huber_loss loss: 0.02480253018438816\n"
     ]
    }
   ],
   "source": [
    "# printing metrics\n",
    "print(f\"Future price after {pred} days is {future_price:.2f}$\")\n",
    "print(f\"{LOSS} loss:\", loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WHdyLI3fjrcZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Stocks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
